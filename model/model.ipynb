{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 00:58:29.302170: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-30 00:58:29.418570: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-30 00:58:29.941615: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/aaaabbee/miniconda3/envs/tf/lib/:/home/aaaabbee/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib\n",
      "2023-04-30 00:58:29.943708: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/aaaabbee/miniconda3/envs/tf/lib/:/home/aaaabbee/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib\n",
      "2023-04-30 00:58:29.943721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-30 00:58:31.332121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-30 00:58:31.360574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-30 00:58:31.361061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from tensorflow.keras import mixed_precision\n",
    "import csv\n",
    "import tensorflow_hub as hub\n",
    "import dataprocessing\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_augmented_images(batch_size,\n",
    "                              img_size,\n",
    "                              normalize=True,\n",
    "                              data_format=\"channels_last\",\n",
    "                              train_location=\"dataset/train_images/\",\n",
    "                              valsplit=True):\n",
    "\n",
    "    if (normalize == True):\n",
    "        aug_gens = ImageDataGenerator(\n",
    "            rescale=1.0 / 255,\n",
    "            featurewise_center=False,\n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            samplewise_std_normalization=False,\n",
    "            zca_whitening=False,\n",
    "            validation_split=0.1,\n",
    "            rotation_range=10,\n",
    "            shear_range=0.15,\n",
    "            zoom_range=0.1,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=False,\n",
    "            vertical_flip=False,\n",
    "            data_format=data_format,\n",
    "            brightness_range=(0.9, 1.1),\n",
    "            fill_mode=\"constant\",\n",
    "        )\n",
    "    else:\n",
    "        aug_gens = ImageDataGenerator(\n",
    "            featurewise_center=False,\n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            samplewise_std_normalization=False,\n",
    "            zca_whitening=False,\n",
    "            validation_split=0.1,\n",
    "            rotation_range=10,\n",
    "            shear_range=0.15,\n",
    "            zoom_range=0.1,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=False,\n",
    "            vertical_flip=False,\n",
    "            data_format=data_format,\n",
    "            brightness_range=(0.9, 1.1),\n",
    "            fill_mode=\"constant\",\n",
    "        )\n",
    "    if (valsplit):\n",
    "\n",
    "        train_data = aug_gens.flow_from_directory(train_location,\n",
    "                                                  subset=\"training\",\n",
    "                                                  seed=1447,\n",
    "                                                  target_size=(img_size,\n",
    "                                                               img_size),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode=\"categorical\")\n",
    "\n",
    "        val_data = aug_gens.flow_from_directory(train_location,\n",
    "                                                subset=\"validation\",\n",
    "                                                seed=1447,\n",
    "                                                target_size=(img_size,\n",
    "                                                             img_size),\n",
    "                                                batch_size=batch_size,\n",
    "                                                class_mode=\"categorical\")\n",
    "    else:\n",
    "        train_data = aug_gens.flow_from_directory(train_location,\n",
    "                                                  subset=None,\n",
    "                                                  seed=1447,\n",
    "                                                  target_size=(img_size,\n",
    "                                                               img_size),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode=\"categorical\")\n",
    "\n",
    "        val_data = None\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def generate_nonaugmented_images(batch_size,\n",
    "                                 img_size,\n",
    "                                 normalize=True,\n",
    "                                 train_location=\"dataset/train_images/\",\n",
    "                                 valsplit=True,\n",
    "                                 class_mode=\"categorical\"):\n",
    "\n",
    "    if (normalize == True):\n",
    "        aug_gens = ImageDataGenerator(\n",
    "            rescale=1.0 / 255,\n",
    "            validation_split=0.1,\n",
    "        )\n",
    "    else:\n",
    "        aug_gens = ImageDataGenerator(validation_split=0.1)\n",
    "    if (valsplit):\n",
    "        train_data = aug_gens.flow_from_directory(train_location,\n",
    "                                                  subset=\"training\",\n",
    "                                                  seed=1447,\n",
    "                                                  target_size=(img_size,\n",
    "                                                               img_size),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode=class_mode)\n",
    "\n",
    "        val_data = aug_gens.flow_from_directory(train_location,\n",
    "                                                subset=\"validation\",\n",
    "                                                seed=1447,\n",
    "                                                target_size=(img_size,\n",
    "                                                             img_size),\n",
    "                                                batch_size=batch_size,\n",
    "                                                class_mode=class_mode)\n",
    "    else:\n",
    "        train_data = aug_gens.flow_from_directory(train_location,\n",
    "                                                  subset=None,\n",
    "                                                  seed=1447,\n",
    "                                                  target_size=(img_size,\n",
    "                                                               img_size),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode=class_mode)\n",
    "        val_data = None\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def generate_test_labels(test_location=\"dataset/test_images\"):\n",
    "\n",
    "    img_id = []\n",
    "    for fileName in os.listdir(test_location):\n",
    "        img_id.append(fileName)\n",
    "    return img_id\n",
    "\n",
    "\n",
    "def tta_prediction(model,\n",
    "                   batch_size,\n",
    "                   img_size,\n",
    "                   normalize=True,\n",
    "                   data_format='channels_last',\n",
    "                   test_location=\"dataset/test_images\"):\n",
    "\n",
    "    if (normalize == True):\n",
    "        aug_gens = ImageDataGenerator(\n",
    "            rescale=1.0 / 255,\n",
    "            featurewise_center=False,\n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            samplewise_std_normalization=False,\n",
    "            zca_whitening=False,\n",
    "            validation_split=0.1,\n",
    "            rotation_range=10,\n",
    "            shear_range=0.25,\n",
    "            zoom_range=0.1,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=False,\n",
    "            vertical_flip=False,\n",
    "            data_format=data_format,\n",
    "            fill_mode=\"constant\",\n",
    "        )\n",
    "    else:\n",
    "        aug_gens = ImageDataGenerator(\n",
    "            featurewise_center=False,\n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            samplewise_std_normalization=False,\n",
    "            zca_whitening=False,\n",
    "            validation_split=0.1,\n",
    "            rotation_range=10,\n",
    "            shear_range=0.25,\n",
    "            zoom_range=0.1,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=False,\n",
    "            vertical_flip=False,\n",
    "            data_format=data_format,\n",
    "            fill_mode=\"constant\",\n",
    "        )\n",
    "\n",
    "    tta_steps = 10\n",
    "    predictions = []\n",
    "    for i in tqdm(range(tta_steps)):\n",
    "        preds = model.predict(aug_gens.flow_from_directory(\n",
    "            directory=test_location,\n",
    "            target_size=(img_size, img_size),\n",
    "            batch_size=batch_size,\n",
    "            classes=['.'],\n",
    "            shuffle=False,\n",
    "        ),\n",
    "                              workers=16)\n",
    "        predictions.append(preds)\n",
    "\n",
    "    final_pred = np.mean(predictions, axis=0)\n",
    "    return final_pred\n",
    "\n",
    "\n",
    "# This does not work\n",
    "def get_data(batch_size, img_size):\n",
    "    traindf = pd.read_csv(\"otherdataset/train/_annotations.csv\", dtype=str)\n",
    "    valdf = pd.read_csv(\"otherdataset/valid/_annotations.csv\", dtype=str)\n",
    "    testdf = pd.read_csv(\"otherdataset/test/_annotations.csv\", dtype=str)\n",
    "\n",
    "    aug_gens = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=10,\n",
    "        shear_range=0.25,\n",
    "        zoom_range=0.1,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False,\n",
    "        data_format='channels_first',\n",
    "        fill_mode='nearest',\n",
    "    )\n",
    "\n",
    "    train = aug_gens.flow_from_dataframe(dataframe=traindf,\n",
    "                                         directory=\"otherdataset/train/\",\n",
    "                                         x_col='filename',\n",
    "                                         y_col='class',\n",
    "                                         batch_size=batch_size,\n",
    "                                         seed=1447,\n",
    "                                         class_mode='categorical',\n",
    "                                         target_size=(img_size, img_size))\n",
    "\n",
    "    valid = aug_gens.flow_from_dataframe(dataframe=valdf,\n",
    "                                         directory=\"otherdataset/valid/\",\n",
    "                                         x_col='filename',\n",
    "                                         y_col='class',\n",
    "                                         batch_size=batch_size,\n",
    "                                         seed=1447,\n",
    "                                         class_mode='categorical',\n",
    "                                         target_size=(img_size, img_size))\n",
    "\n",
    "    test = aug_gens.flow_from_dataframe(dataframe=testdf,\n",
    "                                        directory=\"otherdataset/test/\",\n",
    "                                        x_col='filename',\n",
    "                                        y_col='class',\n",
    "                                        batch_size=batch_size,\n",
    "                                        seed=1447,\n",
    "                                        class_mode='categorical',\n",
    "                                        target_size=(img_size, img_size))\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'B': 1,\n",
       " 'C': 2,\n",
       " 'D': 3,\n",
       " 'E': 4,\n",
       " 'F': 5,\n",
       " 'G': 6,\n",
       " 'H': 7,\n",
       " 'I': 8,\n",
       " 'J': 9,\n",
       " 'K': 10,\n",
       " 'L': 11,\n",
       " 'M': 12,\n",
       " 'N': 13,\n",
       " 'O': 14,\n",
       " 'P': 15,\n",
       " 'Q': 16,\n",
       " 'R': 17,\n",
       " 'S': 18,\n",
       " 'T': 19,\n",
       " 'U': 20,\n",
       " 'V': 21,\n",
       " 'W': 22,\n",
       " 'X': 23,\n",
       " 'Y': 24,\n",
       " 'Z': 25,\n",
       " '_': 26}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = {}\n",
    "for i, line in enumerate(open(\"wnids.txt\", \"r\")):\n",
    "    label_dict[line.rstrip(\"\\n\")] = int(i)\n",
    "\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "img_size = 224\n",
    "num_classes = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1503 images belonging to 26 classes.\n",
      "Found 153 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = generate_augmented_images(\n",
    "        batch_size,\n",
    "        img_size,\n",
    "        normalize=False,\n",
    "        train_location=\"otherdataset/train_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = dataprocessing.generate_test_labels(\n",
    "        test_location=\"otherdataset/test_images\")\n",
    "test_int = [label_dict[x[0]] for x in test_labels]\n",
    "test_int"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 00:58:31.996604: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-30 00:58:31.998544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-30 00:58:31.999084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-30 00:58:31.999434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-30 00:58:32.695137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-30 00:58:32.695486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-30 00:58:32.695499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-30 00:58:32.695919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-30 00:58:32.695968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7373 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "mobilenet_v2 = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "feature_extractor_model = mobilenet_v2\n",
    "\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_model,\n",
    "                                             input_shape=(img_size, img_size,\n",
    "                                                          3),\n",
    "                                             trainable=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " keras_layer (KerasLayer)    (None, 1280)              2257984   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              1311744   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 26)                6682      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,838,810\n",
      "Trainable params: 3,804,698\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Optimized Neural Network\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Model Layers\n",
    "model.add(keras.layers.Rescaling((1. / 255)))\n",
    "model.add(feature_extractor_layer)\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1024, activation='swish'))\n",
    "model.add(keras.layers.Dense(256, activation='swish'))\n",
    "model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.build(input_shape=(None, img_size, img_size, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 00:58:44.319012: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n",
      "2023-04-30 00:58:45.207415: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 22s 493ms/step - loss: 1.7672 - accuracy: 0.5170 - val_loss: 5.8055 - val_accuracy: 0.2484 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 12s 376ms/step - loss: 0.5575 - accuracy: 0.8510 - val_loss: 9.9915 - val_accuracy: 0.1634 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 13s 367ms/step - loss: 0.4240 - accuracy: 0.8902 - val_loss: 10.7637 - val_accuracy: 0.2745 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 14s 394ms/step - loss: 0.3860 - accuracy: 0.9069 - val_loss: 3.7211 - val_accuracy: 0.4248 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 13s 367ms/step - loss: 0.3184 - accuracy: 0.9301 - val_loss: 4.8460 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 14s 402ms/step - loss: 0.1984 - accuracy: 0.9681 - val_loss: 3.1012 - val_accuracy: 0.6078 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 14s 400ms/step - loss: 0.1792 - accuracy: 0.9727 - val_loss: 1.9247 - val_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 13s 369ms/step - loss: 0.1844 - accuracy: 0.9741 - val_loss: 2.6039 - val_accuracy: 0.6471 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 14s 381ms/step - loss: 0.2535 - accuracy: 0.9581 - val_loss: 2.7099 - val_accuracy: 0.5686 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 15s 397ms/step - loss: 0.2833 - accuracy: 0.9501 - val_loss: 1.6605 - val_accuracy: 0.6536 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 13s 370ms/step - loss: 0.1830 - accuracy: 0.9661 - val_loss: 2.1313 - val_accuracy: 0.6732 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 14s 373ms/step - loss: 0.1778 - accuracy: 0.9747 - val_loss: 3.1145 - val_accuracy: 0.5621 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 14s 402ms/step - loss: 0.1784 - accuracy: 0.9787 - val_loss: 1.2067 - val_accuracy: 0.7582 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 13s 374ms/step - loss: 0.2806 - accuracy: 0.9514 - val_loss: 1.5856 - val_accuracy: 0.7124 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 13s 362ms/step - loss: 0.1619 - accuracy: 0.9807 - val_loss: 1.3992 - val_accuracy: 0.7255 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 14s 397ms/step - loss: 0.1382 - accuracy: 0.9840 - val_loss: 0.9196 - val_accuracy: 0.7712 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 14s 404ms/step - loss: 0.1993 - accuracy: 0.9754 - val_loss: 0.6705 - val_accuracy: 0.7908 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 13s 371ms/step - loss: 0.2126 - accuracy: 0.9641 - val_loss: 1.0642 - val_accuracy: 0.7843 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 13s 369ms/step - loss: 0.2453 - accuracy: 0.9607 - val_loss: 4.4914 - val_accuracy: 0.3856 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 13s 369ms/step - loss: 0.1914 - accuracy: 0.9767 - val_loss: 1.8102 - val_accuracy: 0.6797 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1555 - accuracy: 0.9807\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "24/24 [==============================] - 14s 369ms/step - loss: 0.1555 - accuracy: 0.9807 - val_loss: 1.8124 - val_accuracy: 0.6471 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 13s 367ms/step - loss: 0.1458 - accuracy: 0.9860 - val_loss: 1.0931 - val_accuracy: 0.8039 - lr: 4.0000e-04\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 14s 371ms/step - loss: 0.1153 - accuracy: 0.9920 - val_loss: 0.7585 - val_accuracy: 0.8497 - lr: 4.0000e-04\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 14s 402ms/step - loss: 0.1064 - accuracy: 0.9967 - val_loss: 0.6384 - val_accuracy: 0.8627 - lr: 4.0000e-04\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 13s 403ms/step - loss: 0.0976 - accuracy: 0.9987 - val_loss: 0.4788 - val_accuracy: 0.9150 - lr: 4.0000e-04\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 13s 396ms/step - loss: 0.0959 - accuracy: 0.9993 - val_loss: 0.2664 - val_accuracy: 0.9477 - lr: 4.0000e-04\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 13s 369ms/step - loss: 0.0980 - accuracy: 0.9987 - val_loss: 0.3303 - val_accuracy: 0.9216 - lr: 4.0000e-04\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 13s 365ms/step - loss: 0.0952 - accuracy: 1.0000 - val_loss: 0.3542 - val_accuracy: 0.9346 - lr: 4.0000e-04\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - 14s 401ms/step - loss: 0.0955 - accuracy: 1.0000 - val_loss: 0.2417 - val_accuracy: 0.9412 - lr: 4.0000e-04\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 13s 448ms/step - loss: 0.0942 - accuracy: 1.0000 - val_loss: 0.3704 - val_accuracy: 0.9346 - lr: 4.0000e-04\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 14s 367ms/step - loss: 0.0941 - accuracy: 1.0000 - val_loss: 0.2512 - val_accuracy: 0.9673 - lr: 4.0000e-04\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 14s 373ms/step - loss: 0.0959 - accuracy: 0.9993 - val_loss: 0.3232 - val_accuracy: 0.9412 - lr: 4.0000e-04\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0952 - accuracy: 0.9993\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n",
      "24/24 [==============================] - 13s 376ms/step - loss: 0.0952 - accuracy: 0.9993 - val_loss: 0.5725 - val_accuracy: 0.9020 - lr: 4.0000e-04\n",
      "Epoch 34/100\n",
      "24/24 [==============================] - 14s 368ms/step - loss: 0.1113 - accuracy: 0.9980 - val_loss: 0.3076 - val_accuracy: 0.9412 - lr: 1.6000e-04\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - 13s 364ms/step - loss: 0.0938 - accuracy: 1.0000 - val_loss: 0.3603 - val_accuracy: 0.9346 - lr: 1.6000e-04\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - 13s 359ms/step - loss: 0.0938 - accuracy: 1.0000 - val_loss: 0.3949 - val_accuracy: 0.9150 - lr: 1.6000e-04\n",
      "Epoch 37/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 1.0000\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "24/24 [==============================] - 14s 380ms/step - loss: 0.0940 - accuracy: 1.0000 - val_loss: 0.3233 - val_accuracy: 0.9608 - lr: 1.6000e-04\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "num_epochs = 100\n",
    "lr_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                     patience=4,\n",
    "                                                     verbose=1,\n",
    "                                                     factor=0.4,\n",
    "                                                     min_lr=0.0001)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               min_delta=0.00001,\n",
    "                                               patience=8,\n",
    "                                               mode='auto',\n",
    "                                               restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_data,\n",
    "                        workers=16,\n",
    "                        epochs=num_epochs,\n",
    "                        validation_data=val_data,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stop, lr_reduction],\n",
    "                        max_queue_size=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final validation accuracy of 96% good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp74_ahuhw/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp74_ahuhw/assets\n",
      "2023-04-30 01:12:53.716862: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-04-30 01:12:53.716907: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-04-30 01:12:53.717594: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp74_ahuhw\n",
      "2023-04-30 01:12:53.730292: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-04-30 01:12:53.730321: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmp74_ahuhw\n",
      "2023-04-30 01:12:53.772520: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2023-04-30 01:12:53.789376: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-04-30 01:12:54.531666: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmp74_ahuhw\n",
      "2023-04-30 01:12:54.651897: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 934305 microseconds.\n",
      "2023-04-30 01:12:55.062887: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open('asl.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
