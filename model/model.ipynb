{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 18:01:16.447461: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-03 18:01:16.593176: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-03 18:01:17.113392: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/aaaabbee/miniconda3/envs/tf/lib/:/home/aaaabbee/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib\n",
      "2023-05-03 18:01:17.115177: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/aaaabbee/miniconda3/envs/tf/lib/:/home/aaaabbee/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib\n",
      "2023-05-03 18:01:17.115186: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-03 18:01:18.464592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-03 18:01:18.503519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-03 18:01:18.504066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from tensorflow.keras import mixed_precision\n",
    "import csv\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_augmented_images(batch_size,\n",
    "                              img_size,\n",
    "                              normalize=True,\n",
    "                              data_format=\"channels_last\",\n",
    "                              train_location=\"dataset/train_images/\",\n",
    "                              valsplit=True):\n",
    "\n",
    "    if (normalize == True):\n",
    "        aug_gens = ImageDataGenerator(\n",
    "            rescale=1.0 / 255,\n",
    "            featurewise_center=False,\n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            samplewise_std_normalization=False,\n",
    "            zca_whitening=False,\n",
    "            validation_split=0.1,\n",
    "            rotation_range=10,\n",
    "            shear_range=0.15,\n",
    "            zoom_range=0.1,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=False,\n",
    "            vertical_flip=False,\n",
    "            data_format=data_format,\n",
    "            brightness_range=(0.9, 1.1),\n",
    "            fill_mode=\"constant\",\n",
    "        )\n",
    "    else:\n",
    "        aug_gens = ImageDataGenerator(\n",
    "            featurewise_center=False,\n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            samplewise_std_normalization=False,\n",
    "            zca_whitening=False,\n",
    "            validation_split=0.1,\n",
    "            rotation_range=10,\n",
    "            shear_range=0.15,\n",
    "            zoom_range=0.1,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=False,\n",
    "            vertical_flip=False,\n",
    "            data_format=data_format,\n",
    "            brightness_range=(0.9, 1.1),\n",
    "            fill_mode=\"constant\",\n",
    "        )\n",
    "    if (valsplit):\n",
    "\n",
    "        train_data = aug_gens.flow_from_directory(train_location,\n",
    "                                                  subset=\"training\",\n",
    "                                                  seed=1447,\n",
    "                                                  target_size=(img_size,\n",
    "                                                               img_size),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode=\"categorical\")\n",
    "\n",
    "        val_data = aug_gens.flow_from_directory(train_location,\n",
    "                                                subset=\"validation\",\n",
    "                                                seed=1447,\n",
    "                                                target_size=(img_size,\n",
    "                                                             img_size),\n",
    "                                                batch_size=batch_size,\n",
    "                                                class_mode=\"categorical\")\n",
    "    else:\n",
    "        train_data = aug_gens.flow_from_directory(train_location,\n",
    "                                                  subset=None,\n",
    "                                                  seed=1447,\n",
    "                                                  target_size=(img_size,\n",
    "                                                               img_size),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode=\"categorical\")\n",
    "\n",
    "        val_data = None\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def generate_nonaugmented_images(batch_size,\n",
    "                                 img_size,\n",
    "                                 normalize=True,\n",
    "                                 train_location=\"dataset/train_images/\",\n",
    "                                 valsplit=True,\n",
    "                                 class_mode=\"categorical\"):\n",
    "\n",
    "    if (normalize == True):\n",
    "        aug_gens = ImageDataGenerator(\n",
    "            rescale=1.0 / 255,\n",
    "            validation_split=0.1,\n",
    "        )\n",
    "    else:\n",
    "        aug_gens = ImageDataGenerator(validation_split=0.1)\n",
    "    if (valsplit):\n",
    "        train_data = aug_gens.flow_from_directory(train_location,\n",
    "                                                  subset=\"training\",\n",
    "                                                  seed=1447,\n",
    "                                                  target_size=(img_size,\n",
    "                                                               img_size),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode=class_mode)\n",
    "\n",
    "        val_data = aug_gens.flow_from_directory(train_location,\n",
    "                                                subset=\"validation\",\n",
    "                                                seed=1447,\n",
    "                                                target_size=(img_size,\n",
    "                                                             img_size),\n",
    "                                                batch_size=batch_size,\n",
    "                                                class_mode=class_mode)\n",
    "    else:\n",
    "        train_data = aug_gens.flow_from_directory(train_location,\n",
    "                                                  subset=None,\n",
    "                                                  seed=1447,\n",
    "                                                  target_size=(img_size,\n",
    "                                                               img_size),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode=class_mode)\n",
    "        val_data = None\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def generate_test_labels(test_location=\"dataset/test_images\"):\n",
    "\n",
    "    img_id = []\n",
    "    for fileName in os.listdir(test_location):\n",
    "        img_id.append(fileName)\n",
    "    return img_id\n",
    "\n",
    "\n",
    "def tta_prediction(model,\n",
    "                   batch_size,\n",
    "                   img_size,\n",
    "                   normalize=True,\n",
    "                   data_format='channels_last',\n",
    "                   test_location=\"dataset/test_images\"):\n",
    "\n",
    "    if (normalize == True):\n",
    "        aug_gens = ImageDataGenerator(\n",
    "            rescale=1.0 / 255,\n",
    "            featurewise_center=False,\n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            samplewise_std_normalization=False,\n",
    "            zca_whitening=False,\n",
    "            validation_split=0.1,\n",
    "            rotation_range=10,\n",
    "            shear_range=0.25,\n",
    "            zoom_range=0.1,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=False,\n",
    "            vertical_flip=False,\n",
    "            data_format=data_format,\n",
    "            fill_mode=\"constant\",\n",
    "        )\n",
    "    else:\n",
    "        aug_gens = ImageDataGenerator(\n",
    "            featurewise_center=False,\n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            samplewise_std_normalization=False,\n",
    "            zca_whitening=False,\n",
    "            validation_split=0.1,\n",
    "            rotation_range=10,\n",
    "            shear_range=0.25,\n",
    "            zoom_range=0.1,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=False,\n",
    "            vertical_flip=False,\n",
    "            data_format=data_format,\n",
    "            fill_mode=\"constant\",\n",
    "        )\n",
    "\n",
    "    tta_steps = 10\n",
    "    predictions = []\n",
    "    for i in tqdm(range(tta_steps)):\n",
    "        preds = model.predict(aug_gens.flow_from_directory(\n",
    "            directory=test_location,\n",
    "            target_size=(img_size, img_size),\n",
    "            batch_size=batch_size,\n",
    "            classes=['.'],\n",
    "            shuffle=False,\n",
    "        ),\n",
    "                              workers=16)\n",
    "        predictions.append(preds)\n",
    "\n",
    "    final_pred = np.mean(predictions, axis=0)\n",
    "    return final_pred\n",
    "\n",
    "\n",
    "# This does not work\n",
    "def get_data(batch_size, img_size):\n",
    "    traindf = pd.read_csv(\"otherdataset/train/_annotations.csv\", dtype=str)\n",
    "    valdf = pd.read_csv(\"otherdataset/valid/_annotations.csv\", dtype=str)\n",
    "    testdf = pd.read_csv(\"otherdataset/test/_annotations.csv\", dtype=str)\n",
    "\n",
    "    aug_gens = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=10,\n",
    "        shear_range=0.25,\n",
    "        zoom_range=0.1,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False,\n",
    "        data_format='channels_first',\n",
    "        fill_mode='nearest',\n",
    "    )\n",
    "\n",
    "    train = aug_gens.flow_from_dataframe(dataframe=traindf,\n",
    "                                         directory=\"otherdataset/train/\",\n",
    "                                         x_col='filename',\n",
    "                                         y_col='class',\n",
    "                                         batch_size=batch_size,\n",
    "                                         seed=1447,\n",
    "                                         class_mode='categorical',\n",
    "                                         target_size=(img_size, img_size))\n",
    "\n",
    "    valid = aug_gens.flow_from_dataframe(dataframe=valdf,\n",
    "                                         directory=\"otherdataset/valid/\",\n",
    "                                         x_col='filename',\n",
    "                                         y_col='class',\n",
    "                                         batch_size=batch_size,\n",
    "                                         seed=1447,\n",
    "                                         class_mode='categorical',\n",
    "                                         target_size=(img_size, img_size))\n",
    "\n",
    "    test = aug_gens.flow_from_dataframe(dataframe=testdf,\n",
    "                                        directory=\"otherdataset/test/\",\n",
    "                                        x_col='filename',\n",
    "                                        y_col='class',\n",
    "                                        batch_size=batch_size,\n",
    "                                        seed=1447,\n",
    "                                        class_mode='categorical',\n",
    "                                        target_size=(img_size, img_size))\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'B': 1,\n",
       " 'C': 2,\n",
       " 'D': 3,\n",
       " 'E': 4,\n",
       " 'F': 5,\n",
       " 'G': 6,\n",
       " 'H': 7,\n",
       " 'I': 8,\n",
       " 'J': 9,\n",
       " 'K': 10,\n",
       " 'L': 11,\n",
       " 'M': 12,\n",
       " 'N': 13,\n",
       " 'O': 14,\n",
       " 'P': 15,\n",
       " 'Q': 16,\n",
       " 'R': 17,\n",
       " 'S': 18,\n",
       " 'T': 19,\n",
       " 'U': 20,\n",
       " 'V': 21,\n",
       " 'W': 22,\n",
       " 'X': 23,\n",
       " 'Y': 24,\n",
       " 'Z': 25,\n",
       " '_': 26}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = {}\n",
    "for i, line in enumerate(open(\"wnids.txt\", \"r\")):\n",
    "    label_dict[line.rstrip(\"\\n\")] = int(i)\n",
    "\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "img_size = 224\n",
    "num_classes = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1503 images belonging to 26 classes.\n",
      "Found 153 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = generate_augmented_images(\n",
    "        batch_size,\n",
    "        img_size,\n",
    "        normalize=False,\n",
    "        train_location=\"otherdataset/train_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = generate_test_labels(\n",
    "        test_location=\"otherdataset/test_images\")\n",
    "test_int = [label_dict[x[0]] for x in test_labels]\n",
    "test_int"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 18:01:19.208754: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-03 18:01:19.210407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-03 18:01:19.211090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-03 18:01:19.211708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-03 18:01:19.883937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-03 18:01:19.884664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-03 18:01:19.884680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-05-03 18:01:19.885221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-03 18:01:19.885308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7373 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "mobilenet_v2 = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "feature_extractor_model = mobilenet_v2\n",
    "\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_model,\n",
    "                                             input_shape=(img_size, img_size,\n",
    "                                                          3),\n",
    "                                             trainable=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " keras_layer (KerasLayer)    (None, 1280)              2257984   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              1311744   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 26)                6682      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,838,810\n",
      "Trainable params: 3,804,698\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Optimized Neural Network\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Model Layers\n",
    "model.add(keras.layers.Rescaling((1. / 255)))\n",
    "model.add(feature_extractor_layer)\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1024, activation='swish'))\n",
    "model.add(keras.layers.Dense(256, activation='swish'))\n",
    "model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.build(input_shape=(None, img_size, img_size, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 18:01:31.161640: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n",
      "2023-05-03 18:01:32.255998: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 24s 582ms/step - loss: 1.9677 - accuracy: 0.4551 - val_loss: 4.6221 - val_accuracy: 0.4183 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 14s 421ms/step - loss: 0.6638 - accuracy: 0.8237 - val_loss: 3.0993 - val_accuracy: 0.3791 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 14s 418ms/step - loss: 0.3634 - accuracy: 0.9202 - val_loss: 2.6763 - val_accuracy: 0.5686 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 13s 379ms/step - loss: 0.3790 - accuracy: 0.9162 - val_loss: 3.1565 - val_accuracy: 0.4379 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 15s 420ms/step - loss: 0.2762 - accuracy: 0.9408 - val_loss: 2.5392 - val_accuracy: 0.6209 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 16s 520ms/step - loss: 0.2063 - accuracy: 0.9681 - val_loss: 1.8691 - val_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 15s 443ms/step - loss: 0.1874 - accuracy: 0.9707 - val_loss: 1.8959 - val_accuracy: 0.6340 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 15s 425ms/step - loss: 0.2187 - accuracy: 0.9621 - val_loss: 2.7171 - val_accuracy: 0.6340 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 15s 424ms/step - loss: 0.3383 - accuracy: 0.9315 - val_loss: 2.3891 - val_accuracy: 0.6209 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 16s 460ms/step - loss: 0.2304 - accuracy: 0.9581 - val_loss: 1.8607 - val_accuracy: 0.6797 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 14s 397ms/step - loss: 0.1952 - accuracy: 0.9661 - val_loss: 2.6372 - val_accuracy: 0.6275 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 14s 391ms/step - loss: 0.3029 - accuracy: 0.9494 - val_loss: 2.9056 - val_accuracy: 0.6078 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 14s 388ms/step - loss: 0.3245 - accuracy: 0.9441 - val_loss: 2.8123 - val_accuracy: 0.5621 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3051 - accuracy: 0.9421\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "24/24 [==============================] - 14s 380ms/step - loss: 0.3051 - accuracy: 0.9421 - val_loss: 2.9056 - val_accuracy: 0.5621 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 14s 487ms/step - loss: 0.1540 - accuracy: 0.9800 - val_loss: 1.9326 - val_accuracy: 0.7124 - lr: 4.0000e-04\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 15s 410ms/step - loss: 0.1038 - accuracy: 0.9993 - val_loss: 1.0987 - val_accuracy: 0.7712 - lr: 4.0000e-04\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 15s 479ms/step - loss: 0.1018 - accuracy: 0.9980 - val_loss: 0.8412 - val_accuracy: 0.8497 - lr: 4.0000e-04\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 15s 434ms/step - loss: 0.0970 - accuracy: 0.9993 - val_loss: 0.7279 - val_accuracy: 0.8562 - lr: 4.0000e-04\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 14s 403ms/step - loss: 0.0969 - accuracy: 0.9987 - val_loss: 0.5813 - val_accuracy: 0.8889 - lr: 4.0000e-04\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 14s 399ms/step - loss: 0.1022 - accuracy: 0.9980 - val_loss: 0.5845 - val_accuracy: 0.8954 - lr: 4.0000e-04\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 15s 418ms/step - loss: 0.0965 - accuracy: 0.9987 - val_loss: 0.5090 - val_accuracy: 0.8889 - lr: 4.0000e-04\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 14s 400ms/step - loss: 0.0979 - accuracy: 0.9987 - val_loss: 0.6496 - val_accuracy: 0.8627 - lr: 4.0000e-04\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 14s 395ms/step - loss: 0.1018 - accuracy: 0.9973 - val_loss: 0.6176 - val_accuracy: 0.8824 - lr: 4.0000e-04\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 15s 422ms/step - loss: 0.0976 - accuracy: 0.9987 - val_loss: 0.5246 - val_accuracy: 0.8954 - lr: 4.0000e-04\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 16s 460ms/step - loss: 0.0986 - accuracy: 0.9987 - val_loss: 0.4168 - val_accuracy: 0.9085 - lr: 4.0000e-04\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 14s 431ms/step - loss: 0.0970 - accuracy: 0.9987 - val_loss: 0.5619 - val_accuracy: 0.8758 - lr: 4.0000e-04\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 15s 394ms/step - loss: 0.1013 - accuracy: 0.9980 - val_loss: 0.4882 - val_accuracy: 0.8954 - lr: 4.0000e-04\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 14s 379ms/step - loss: 0.0993 - accuracy: 0.9980 - val_loss: 0.5715 - val_accuracy: 0.9150 - lr: 4.0000e-04\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9967\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n",
      "24/24 [==============================] - 14s 400ms/step - loss: 0.1012 - accuracy: 0.9967 - val_loss: 0.6378 - val_accuracy: 0.8301 - lr: 4.0000e-04\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 14s 387ms/step - loss: 0.0971 - accuracy: 0.9987 - val_loss: 0.5861 - val_accuracy: 0.8693 - lr: 1.6000e-04\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 15s 423ms/step - loss: 0.1027 - accuracy: 0.9980 - val_loss: 0.5580 - val_accuracy: 0.8627 - lr: 1.6000e-04\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 15s 424ms/step - loss: 0.0933 - accuracy: 1.0000 - val_loss: 0.5948 - val_accuracy: 0.8954 - lr: 1.6000e-04\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 1.0000\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "24/24 [==============================] - 16s 448ms/step - loss: 0.0937 - accuracy: 1.0000 - val_loss: 0.4907 - val_accuracy: 0.8824 - lr: 1.6000e-04\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "num_epochs = 100\n",
    "lr_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                     patience=4,\n",
    "                                                     verbose=1,\n",
    "                                                     factor=0.4,\n",
    "                                                     min_lr=0.0001)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               min_delta=0.00001,\n",
    "                                               patience=8,\n",
    "                                               mode='auto',\n",
    "                                               restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_data,\n",
    "                        workers=16,\n",
    "                        epochs=num_epochs,\n",
    "                        validation_data=val_data,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stop, lr_reduction],\n",
    "                        max_queue_size=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final validation accuracy of 96% good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp3bz2trsz/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp3bz2trsz/assets\n",
      "2023-05-03 18:09:51.021489: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-05-03 18:09:51.021543: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-05-03 18:09:51.022280: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp3bz2trsz\n",
      "2023-05-03 18:09:51.036454: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-05-03 18:09:51.036493: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmp3bz2trsz\n",
      "2023-05-03 18:09:51.086420: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2023-05-03 18:09:51.104437: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-05-03 18:09:51.764947: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmp3bz2trsz\n",
      "2023-05-03 18:09:51.887385: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 865132 microseconds.\n",
      "2023-05-03 18:09:52.174514: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open('asl.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: aslSavedModel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: aslSavedModel/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('asl.h5')\n",
    "model.save('aslSavedModel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
